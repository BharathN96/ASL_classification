{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "448fc1fc",
   "metadata": {},
   "source": [
    "# ASL Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febcdc0c",
   "metadata": {},
   "source": [
    "#### Problem Statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85c5594",
   "metadata": {},
   "source": [
    "Detecting and Classifying ASL Alphabets (A-Z)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d92d84",
   "metadata": {},
   "source": [
    "#### Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c35825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import split_folders\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dropout,Dense,Flatten\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from PIL import ImageFile\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input as mp\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e124971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset folder to Train and Validation\n",
    "split_folders.ratio(\"Alphabets\",output=\"Alphabets_split\",seed=1337,ratio=(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e7b9b",
   "metadata": {},
   "source": [
    "#### Data Augmentation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "train_data=train_datagen.flow_from_directory(\"Alphabets_split/train\",target_size=(32, 32),class_mode='categorical',\n",
    "    batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d000831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_datagen=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_data=val_datagen.flow_from_directory(\"Alphabets_split/val\",target_size=(32, 32),class_mode='categorical',\n",
    "    batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852fcd60",
   "metadata": {},
   "source": [
    "#### VGG19 model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecabeacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising the VGG19 model\n",
    "model=VGG19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d47fa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the entire structure of VGG19\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding the Convolution layers of the VGG19 model and setting the input shape same as that of target size during Data Augmentation\n",
    "model=VGG19(input_shape=(32,32,3),include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf76de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the weights of only the dense layers \n",
    "for layers in model.layers:\n",
    "    if('dense' not in layers.name):\n",
    "        layers.trainable=False\n",
    "    if('dense' in layers.name):\n",
    "        layers.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a6507",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Adding the new dense layers to the VGG19 model\n",
    "transfer_model=Sequential([model,\n",
    "                          Flatten(),\n",
    "                          Dense(32,activation='relu'),\n",
    "                          Dense(26,activation='softmax')])\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model using adam optimizer, loss as 'categorical_crossentropy' and metrics as accuracy\n",
    "transfer_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a76785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "my_model=transfer_model.fit_generator(train_data,steps_per_epoch=518 //8,epochs=100,validation_data=val_data,validation_steps=130  //8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.save(\"sign_detection.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8976de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing performance metrics of the model\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(my_model.history['accuracy'],label='Train')\n",
    "plt.plot(my_model.history['val_accuracy'],label='Test')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(my_model.history['loss'],label='Train')\n",
    "plt.plot(my_model.history['val_loss'],label='Test')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da95880e",
   "metadata": {},
   "source": [
    "#### Making a single Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=cv2.imread(\"Alphabets_split/val/A/A626.jpg\")\n",
    "img1=cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
    "test_image = image.load_img('Alphabets_split/val/A/A626.jpg', target_size = (32, 32))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = transfer_model.predict(test_image/255.)\n",
    "prediction=list(train_data.class_indices.keys())[np.argmax(result)]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Actual: A\",fontsize=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Predicted: {}\".format(prediction),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=cv2.imread(\"Alphabets_split/val/B/B691.jpg\")\n",
    "img1=cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
    "test_image = image.load_img('Alphabets_split/val/B/B691.jpg', target_size = (32, 32))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = transfer_model.predict(test_image/255.)\n",
    "prediction=list(train_data.class_indices.keys())[np.argmax(result)]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Actual: B\",fontsize=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Predicted: {}\".format(prediction),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc6a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=cv2.imread(\"Alphabets_split/val/I/I578.jpg\")\n",
    "img1=cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
    "test_image = image.load_img('Alphabets_split/val/I/I578.jpg', target_size = (32, 32))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = transfer_model.predict(test_image/255.)\n",
    "prediction=list(train_data.class_indices.keys())[np.argmax(result)]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Actual: I\",fontsize=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Predicted: {}\".format(prediction),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=cv2.imread(\"Alphabets_split/val/K/K530.jpg\")\n",
    "img1=cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
    "test_image = image.load_img('Alphabets_split/val/K/K530.jpg', target_size = (32, 32))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = transfer_model.predict(test_image/255.)\n",
    "prediction=list(train_data.class_indices.keys())[np.argmax(result)]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Actual: K\",fontsize=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Predicted: {}\".format(prediction),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc926dd",
   "metadata": {},
   "source": [
    "#### Xception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6610f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "train_data=train_datagen.flow_from_directory(\"Alphabets_split/train\",target_size=(128,128),class_mode='categorical',\n",
    "    batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78990c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_data=val_datagen.flow_from_directory(\"Alphabets_split/val\",target_size=(128,128),class_mode='categorical',\n",
    "    batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb6809",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Xception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5515b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569686be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding the Convolution layers of the Xception model and setting the input shape same as that of target size during Data Augmentation\n",
    "model=Xception(input_shape=(128,128,3),include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the weights of only the dense layers \n",
    "for layers in model.layers:\n",
    "    if('dense' not in layers.name):\n",
    "        layers.trainable=False\n",
    "    if('dense' in layers.name):\n",
    "        layers.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e399f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the new dense layers to the Xception model\n",
    "transfer_model=Sequential([model,\n",
    "                          Flatten(),\n",
    "                          Dense(32,activation='relu'),\n",
    "                          Dense(26,activation='softmax')])\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab1bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model using adam optimizer, loss as 'categorical_crossentropy' and metrics as accuracy\n",
    "transfer_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ae59b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "my_model=transfer_model.fit_generator(train_data,steps_per_epoch=518 //8,epochs=100,validation_data=val_data,validation_steps=130  //8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c0f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing performance metrics of the model\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(my_model.history['accuracy'],label='Train')\n",
    "plt.plot(my_model.history['val_accuracy'],label='Test')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(my_model.history['loss'],label='Train')\n",
    "plt.plot(my_model.history['val_loss'],label='Test')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54e08d",
   "metadata": {},
   "source": [
    "#### Make Single Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a288494",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=cv2.imread(\"Alphabets_split/val/A/A626.jpg\")\n",
    "img1=cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
    "test_image = image.load_img('Alphabets_split/val/A/A626.jpg', target_size = (128, 128))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = transfer_model.predict(test_image/255.)\n",
    "prediction=list(train_data.class_indices.keys())[np.argmax(result)]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Actual: A\",fontsize=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Predicted: {}\".format(prediction),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47782188",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=cv2.imread(\"Alphabets_split/val/B/B691.jpg\")\n",
    "img1=cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
    "test_image = image.load_img('Alphabets_split/val/B/B691.jpg', target_size = (128, 128))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = transfer_model.predict(test_image/255.)\n",
    "prediction=list(train_data.class_indices.keys())[np.argmax(result)]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Actual: B\",fontsize=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Predicted: {}\".format(prediction),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=cv2.imread(\"Alphabets_split/val/I/I578.jpg\")\n",
    "img1=cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
    "test_image = image.load_img('Alphabets_split/val/I/I578.jpg', target_size = (128, 128))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = transfer_model.predict(test_image/255.)\n",
    "prediction=list(train_data.class_indices.keys())[np.argmax(result)]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Actual: I\",fontsize=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Predicted: {}\".format(prediction),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8745aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=cv2.imread(\"Alphabets_split/val/K/K530.jpg\")\n",
    "img1=cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
    "test_image = image.load_img('Alphabets_split/val/K/K530.jpg', target_size = (128, 128))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = transfer_model.predict(test_image/255.)\n",
    "prediction=list(train_data.class_indices.keys())[np.argmax(result)]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Actual: K\",fontsize=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Predicted: {}\".format(prediction),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ab5fe",
   "metadata": {},
   "source": [
    "#### Mobile Net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "train_data=train_datagen.flow_from_directory(\"Alphabets_split/train\",target_size=(32, 32),class_mode='categorical',\n",
    "    batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_data=val_datagen.flow_from_directory(\"Alphabets_split/val\",target_size=(32, 32),class_mode='categorical',\n",
    "    batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd46803",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MobileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ae11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding the Convolution layers of the MobileNet model and setting the input shape same as that of target size during Data Augmentation\n",
    "model=MobileNet(input_shape=(32,32,3),include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the weights of only the dense layers \n",
    "for layers in model.layers:\n",
    "    if('dense' not in layers.name):\n",
    "        layers.trainable=False\n",
    "    if('dense' in layers.name):\n",
    "        layers.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a118822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the new dense layers to the MobileNet model\n",
    "transfer_model=Sequential([model,\n",
    "                          Flatten(),\n",
    "                          Dense(32,activation='relu'),\n",
    "                          Dense(26,activation='softmax')])\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model using adam optimizer, loss as 'categorical_crossentropy' and metrics as accuracy\n",
    "transfer_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a48e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "my_model=transfer_model.fit_generator(train_data,steps_per_epoch=518 //8,epochs=100,validation_data=val_data,validation_steps=130  //8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing performance metrics of the model\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(my_model.history['accuracy'],label='Train')\n",
    "plt.plot(my_model.history['val_accuracy'],label='Test')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(my_model.history['loss'],label='Train')\n",
    "plt.plot(my_model.history['val_loss'],label='Test')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8417f4be",
   "metadata": {},
   "source": [
    "#### Make Single Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b220f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=cv2.imread(\"Alphabets_split/val/A/A626.jpg\")\n",
    "img1=cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
    "test_image = image.load_img('Alphabets_split/val/A/A626.jpg', target_size = (32, 32))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = transfer_model.predict(test_image/255.)\n",
    "prediction=list(train_data.class_indices.keys())[np.argmax(result)]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Actual: A\",fontsize=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Predicted: {}\".format(prediction),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=cv2.imread(\"Alphabets_split/val/B/B691.jpg\")\n",
    "img1=cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
    "test_image = image.load_img('Alphabets_split/val/B/B691.jpg', target_size = (32, 32))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = transfer_model.predict(test_image/255.)\n",
    "prediction=list(train_data.class_indices.keys())[np.argmax(result)]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Actual: B\",fontsize=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Predicted: {}\".format(prediction),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ea0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=cv2.imread(\"Alphabets_split/val/I/I578.jpg\")\n",
    "img1=cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
    "test_image = image.load_img('Alphabets_split/val/I/I578.jpg', target_size = (32, 32))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = transfer_model.predict(test_image/255.)\n",
    "prediction=list(train_data.class_indices.keys())[np.argmax(result)]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Actual: I\",fontsize=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Predicted: {}\".format(prediction),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=cv2.imread(\"Alphabets_split/val/K/K530.jpg\")\n",
    "img1=cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
    "test_image = image.load_img('Alphabets_split/val/K/K530.jpg', target_size = (32, 32))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = transfer_model.predict(test_image/255.)\n",
    "prediction=list(train_data.class_indices.keys())[np.argmax(result)]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Actual: K\",fontsize=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Predicted: {}\".format(prediction),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162a8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(\"p_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb44135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifier = cv2.CascadeClassifier('D:/MTech-DSML/DATA/haarcascade_frontalface_alt2.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b50abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(img):\n",
    "    gesture=face_classifier.detectMultiScale(img,2.3,2)\n",
    "    \n",
    "    if gesture is ():\n",
    "        return None\n",
    "    \n",
    "    for (x,y,w,h) in gesture:\n",
    "        x=x-10\n",
    "        y=y-10\n",
    "        cropped=img[y:y+h+150,x:x+w+150]\n",
    "        \n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    _,frame = video_capture.read()\n",
    "    \n",
    "    gest=feature_extractor(frame)\n",
    "    if type(gest) is np.ndarray:\n",
    "        gest = cv2.resize(gest, (32, 32))\n",
    "        im = Image.fromarray(gest, 'RGB')\n",
    "        \n",
    "        img_array = np.array(im)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        pred = model.predict(img_array)\n",
    "        print(pred)\n",
    "        \n",
    "        name=\"None matching\"\n",
    "        \n",
    "        if(pred[0][0]>0.5):\n",
    "            name='A'\n",
    "        elif (pred[0][1]>0.5) :\n",
    "            name='B'\n",
    "        cv2.putText(frame,name, (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "    else:\n",
    "        cv2.putText(frame,\"\", (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f025096",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
